{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Implementation of EEGLearn - P. Bashivan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes a short summary of Pytorch implementation of the models described in \"Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks.\" Bashivan et al. at International conference on learning representations (2016).\n",
    "\n",
    "The rest of the code is in the different python scripts of this repo.\n",
    "\n",
    "All the codes have been inspired from the [original github](https://github.com/pbashivan/EEGLearn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "import torch\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from Utils import *\n",
    "from Models import *\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the original Images \n",
    "The images have directly been taken from original implementation, given that they remain the same nevermind the implementation (Pytorch, Tensorflow, Theano)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2670, 3, 32, 32)\n",
      "(7, 2670, 3, 32, 32)\n",
      "(2670,)\n",
      "(2670,)\n"
     ]
    }
   ],
   "source": [
    "Mean_Images = sio.loadmat(\"Sample Data/images.mat\")[\"img\"] #corresponding to the images mean for all the seven windows\n",
    "print(np.shape(Mean_Images)) \n",
    "Images = sio.loadmat(\"Sample Data/images_time.mat\")[\"img\"] #corresponding to the images mean for all the seven windows\n",
    "print(np.shape(Images)) \n",
    "Label = (sio.loadmat(\"Sample Data/FeatureMat_timeWin\")[\"features\"][:,-1]-1).astype(int) #corresponding to the signal label (i.e. load levels).\n",
    "print(np.shape(Label)) \n",
    "Patient_id = sio.loadmat(\"Sample Data/trials_subNums.mat\")['subjectNum'][0] #corresponding to the patient id\n",
    "print(np.shape(Patient_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a2314234c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write code to plot matrix of dimension 3,32,32 as a color image\n",
    "plt.imshow(Mean_Images[0,0, :,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading patient dataset \n",
    "From the total data, we select the images corresponding patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose among the patient : [ 1  2  3  4  6  7  8  9 10 11 12 14 15]\n"
     ]
    }
   ],
   "source": [
    "print(\"Choose among the patient : \"+str(np.unique(Patient_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_patient = 1\n",
    "# load file from folder kamyar_data\n",
    "files = sio.loadmat(\"kamyar_data/TrainData1.mat\")['TrainData1']\n",
    "Mean_Images = files[:,:]\n",
    "Label = files[:,9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # **************************** Data Segmentation *********************\n",
    "def DataSegmentation(data, t1, t2):\n",
    "    Data_Frame = pd.DataFrame()\n",
    "    df = pd.DataFrame(data)\n",
    "    dt = df.loc[0][1] - df.loc[0][0]\n",
    "    fs = 1/dt\n",
    "    sample_num = np.floor((t1+t2)*fs)\n",
    "    epoch_loc = np.where(np.diff(np.sign(df.loc[9])) == 1)[0] + 1\n",
    "    for j in range(1,9):\n",
    "        signals = []\n",
    "        for i in range(epoch_loc.shape[0]):\n",
    "            temp = df.loc[j][epoch_loc[i]-np.int64(t1*fs) : epoch_loc[i]+np.int64(t2*fs)+1].values\n",
    "            # temp = temp/np.max(abs(temp))\n",
    "            signals.append(temp)\n",
    "        Data_Frame['Ch' + str(j)] = signals\n",
    "    Data_Frame['ChannelMean'] = [[0] for i in range(Data_Frame.shape[0])]\n",
    "    for i in range(Data_Frame.shape[0]):\n",
    "        Data_Frame['ChannelMean'][i] = np.mean(Data_Frame.loc[i][:-1])\n",
    "\n",
    "    try:\n",
    "        target_loc = np.where(df.loc[10] == 1)[0]\n",
    "        labels = np.zeros(epoch_loc.shape)\n",
    "        for i in range(target_loc.shape[0]):\n",
    "            temp = np.where(epoch_loc == target_loc[i])[0]\n",
    "            if temp.shape[0] == 0:\n",
    "                continue\n",
    "            labels[temp[0]] = 1\n",
    "        Data_Frame['labels'] = labels\n",
    "        return (Data_Frame, fs, pd.DataFrame(data).loc[9][epoch_loc].values)\n",
    "    except:\n",
    "        return (Data_Frame, fs, pd.DataFrame(data).loc[9][epoch_loc].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row of mean_images represents a signal in time domain, take the wavelet transform of each row\n",
    "cwts,_,_ = DataSegmentation(Mean_Images, 100/1000, 400/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600, 128)\n"
     ]
    }
   ],
   "source": [
    "signals = cwts.iloc[:,:-2].values\n",
    "signals = signals.flatten()\n",
    "signals = np.array([np.array(signals[i]) for i in range(signals.shape[0])])\n",
    "print (signals.shape)\n",
    "labels = cwts.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each row of signals to wavelet transform matrix\n",
    "wavelets = np.zeros((signals.shape[0], 128, 128))\n",
    "for i in range(signals.shape[0]):\n",
    "    wavelets[i,:,:] = signal.cwt(signals[i,:], signal.ricker, np.arange(1, 129))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2700, 8, 128, 128)\n",
      "(2700,)\n"
     ]
    }
   ],
   "source": [
    "wavelets = wavelets.reshape((wavelets.shape[0], 1, wavelets.shape[1], wavelets.shape[2]))\n",
    "wavelets = wavelets.reshape((int(wavelets.shape[0] / 8), 8, wavelets.shape[2], wavelets.shape[3]))\n",
    "print (wavelets.shape)\n",
    "print (labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: BasicCNN\n",
    "First Implementation of a CNN on the Mean Images from each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part = 0.8\n",
    "test_part = 0.2\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2700, 8, 128, 128)\n",
      "(2700,)\n"
     ]
    }
   ],
   "source": [
    "print (wavelets.shape)\n",
    "print (labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast labels to long int\n",
    "labels = labels.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2625, 8, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# split data into two parts based on labels being 0 or 1\n",
    "wavelets_0 = wavelets[labels == 0]\n",
    "print (wavelets_0.shape)\n",
    "wavelets_1 = wavelets[labels == 1]\n",
    "# shuffle both and corresponding elements of labels\n",
    "inds = np.arange(wavelets_0.shape[0])\n",
    "np.random.shuffle(inds)\n",
    "\n",
    "wavelets_0 = wavelets_0[inds]\n",
    "labels_0 = labels[labels == 0][inds]\n",
    "wavelets_0 = wavelets_0[:75, :, :, :]\n",
    "labels_0 = labels_0[:75]\n",
    "\n",
    "wavelets_1 = wavelets_1[:,:,:,:]\n",
    "labels_1 = labels[labels == 1]\n",
    "\n",
    "# take train_part of each\n",
    "wavelets_0_train = wavelets_0[:int(wavelets_0.shape[0] * train_part)]\n",
    "labels_0_train = labels_0[:int(labels_0.shape[0] * train_part)]\n",
    "wavelets_1_train = wavelets_1[:int(wavelets_1.shape[0] * train_part)]\n",
    "labels_1_train = labels_1[:int(labels_1.shape[0] * train_part)]\n",
    "# take test_part of each\n",
    "wavelets_0_test = wavelets_0[int(wavelets_0.shape[0] * train_part):]\n",
    "labels_0_test = labels_0[int(labels_0.shape[0] * train_part):]\n",
    "wavelets_1_test = wavelets_1[int(wavelets_1.shape[0] * train_part):]\n",
    "labels_1_test = labels_1[int(labels_1.shape[0] * train_part):]\n",
    "# concatenate wavelets_0_train and wavelets_1_train\n",
    "wavelets_train = np.concatenate((wavelets_0_train, wavelets_1_train), axis=0)\n",
    "labels_train = np.concatenate((labels_0_train, labels_1_train), axis=0)\n",
    "# concatenate wavelets_0_test and wavelets_1_test\n",
    "wavelets_test = np.concatenate((wavelets_0_test, wavelets_1_test), axis=0)\n",
    "labels_test = np.concatenate((labels_0_test, labels_1_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG = EEGImagesDataset(label=labels, image=wavelets)\n",
    "\n",
    "lengths = [int(len(EEG)*train_part), int(len(EEG)*test_part)]\n",
    "Train, Test = random_split(EEG, lengths)\n",
    "\n",
    "Trainloader = DataLoader(Train,batch_size=batch_size)\n",
    "Testloader = DataLoader(Test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  15]\tloss: 0.692\tAccuracy : 0.936\t\tval-loss: 0.510\tval-Accuracy : 0.967\n",
      "[2,  15]\tloss: 0.471\tAccuracy : 0.974\t\tval-loss: 0.510\tval-Accuracy : 0.967\n",
      "[3,  15]\tloss: 0.470\tAccuracy : 0.974\t\tval-loss: 0.508\tval-Accuracy : 0.967\n",
      "[4,  15]\tloss: 0.465\tAccuracy : 0.974\t\tval-loss: 0.509\tval-Accuracy : 0.967\n",
      "[5,  15]\tloss: 0.470\tAccuracy : 0.974\t\tval-loss: 0.505\tval-Accuracy : 0.967\n",
      "[6,  15]\tloss: 0.464\tAccuracy : 0.974\t\tval-loss: 0.506\tval-Accuracy : 0.967\n",
      "[7,  15]\tloss: 0.515\tAccuracy : 0.974\t\tval-loss: 0.509\tval-Accuracy : 0.967\n",
      "[8,  15]\tloss: 0.467\tAccuracy : 0.974\t\tval-loss: 0.509\tval-Accuracy : 0.967\n",
      "[9,  15]\tloss: 0.463\tAccuracy : 0.974\t\tval-loss: 0.510\tval-Accuracy : 0.967\n",
      "[10,  15]\tloss: 0.466\tAccuracy : 0.974\t\tval-loss: 0.513\tval-Accuracy : 0.967\n",
      "[11,  15]\tloss: 0.462\tAccuracy : 0.974\t\tval-loss: 0.512\tval-Accuracy : 0.967\n",
      "[12,  15]\tloss: 0.464\tAccuracy : 0.974\t\tval-loss: 0.509\tval-Accuracy : 0.967\n",
      "[13,  15]\tloss: 0.460\tAccuracy : 0.974\t\tval-loss: 0.507\tval-Accuracy : 0.967\n",
      "[14,  15]\tloss: 0.458\tAccuracy : 0.974\t\tval-loss: 0.508\tval-Accuracy : 0.967\n",
      "[15,  15]\tloss: 0.454\tAccuracy : 0.974\t\tval-loss: 0.507\tval-Accuracy : 0.967\n"
     ]
    }
   ],
   "source": [
    "res = TrainTest_Model(BasicCNN, Trainloader, Testloader, n_epoch=15, learning_rate=0.001, print_epoch=-1, opti='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6858674953558982, 0.8851851851851852, 0.5330066786092871, 0.9685185185185186, [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])], [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0])])\n"
     ]
    }
   ],
   "source": [
    "# print accuracy of the model\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxpool CNN\n",
    "Build the Max-pooling model performing a maxpool over the 7 parallel convnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part = 0.8\n",
    "test_part = 0.2\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG = EEGImagesDataset(label=labels, image=wavelets)\n",
    "\n",
    "lengths = [int(len(EEG)*train_part), int(len(EEG)*test_part)]\n",
    "Train, Test = random_split(EEG, lengths)\n",
    "\n",
    "Trainloader = DataLoader(Train,batch_size=batch_size)\n",
    "Testloader = DataLoader(Test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 32, 128, 128] to have 3 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\GitHub\\EEGLearn-Pytorch\\EEGLearn_ShortDemo.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/GitHub/EEGLearn-Pytorch/EEGLearn_ShortDemo.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m res \u001b[39m=\u001b[39m TrainTest_Model(MaxCNN, Trainloader, Testloader, n_epoch\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, print_epoch\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, opti\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mAdam\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\GitHub\\EEGLearn-Pytorch\\Utils.py:117\u001b[0m, in \u001b[0;36mTrainTest_Model\u001b[1;34m(model, trainloader, testloader, n_epoch, opti, learning_rate, is_cuda, print_epoch, verbose)\u001b[0m\n\u001b[0;32m    114\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    116\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat32))\n\u001b[0;32m    118\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[0;32m    119\u001b[0m evaluation\u001b[39m.\u001b[39mappend((predicted\u001b[39m==\u001b[39mlabels)\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\GitHub\\EEGLearn-Pytorch\\Models.py:126\u001b[0m, in \u001b[0;36mMaxCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    124\u001b[0m     tmp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\u001b[39m128\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m7\u001b[39m):\n\u001b[1;32m--> 126\u001b[0m     tmp[:,i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1( F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv7(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv6(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv5(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1( F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3( F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x[:,i])))))))))))))))))\n\u001b[0;32m    127\u001b[0m x \u001b[39m=\u001b[39m tmp\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\u001b[39m4\u001b[39m\u001b[39m*\u001b[39m\u001b[39m128\u001b[39m\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m    128\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 32, 128, 128] to have 3 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "res = TrainTest_Model(MaxCNN, Trainloader, Testloader, n_epoch=1, learning_rate=0.001, print_epoch=5, opti='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp CNN\n",
    "FBuild the Conv1D model performing a convolution1D over the 7 parallel convnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Begin Training for Patient '+str(choosen_patient))\n",
    "res = TrainTest_Model(TempCNN, Trainloader, Testloader, n_epoch=45, learning_rate=0.001, print_epoch=5, opti='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM CNN\n",
    "Build the LSTM model applying a RNN over the 7 parallel convnets outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG = EEGImagesDataset(label=Label[Patient_id==choosen_patient], image=Images[Patient_id==choosen_patient])\n",
    "\n",
    "lengths = [int(len(EEG)*train_part+1), int(len(EEG)*test_part)]\n",
    "Train, Test = random_split(EEG, lengths)\n",
    "\n",
    "Trainloader = DataLoader(Train,batch_size=batch_size)\n",
    "Testloader = DataLoader(Test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Begin Training for Patient '+str(choosen_patient))\n",
    "res = TrainTest_Model(LSTM, Trainloader, Testloader, n_epoch=45, learning_rate=0.0001, print_epoch=5, opti='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mix CNN\n",
    "Build the LSTM model applying a RNN and a CNN over the 7 parallel convnets outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG = EEGImagesDataset(label=Label[Patient_id==choosen_patient], image=Images[Patient_id==choosen_patient])\n",
    "\n",
    "lengths = [int(len(EEG)*train_part+1), int(len(EEG)*test_part)]\n",
    "Train, Test = random_split(EEG, lengths)\n",
    "\n",
    "Trainloader = DataLoader(Train,batch_size=batch_size)\n",
    "Testloader = DataLoader(Test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Begin Training for Patient '+str(choosen_patient))\n",
    "res = TrainTest_Model(Mix, Trainloader, Testloader, n_epoch=60, learning_rate=0.00001, print_epoch=5, opti='Adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
